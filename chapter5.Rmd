---
title: "Chapter 5 - Dimensionality reduction techniques"
author: "Gyan Dookie"
date: "22 February 2017"
output: html_document
---
```{r, echo=FALSE, message=FALSE}

#install.packages("dplyr")
#install.packages("corrplot")

#install.packages("tidyr")
library(tidyr)
library(dplyr)
library(corrplot)
library(tidyr)

library(ggplot2)
library(GGally)

# Load the ‘human’ data into R. Explore the structure and the dimensions of the data and describe the dataset briefly, assuming the reader has no previous knowledge of it (this is now close to the reality, since you have named the variables yourself). (0-1 point)

#human <- read.table("data/human.csv", header= TRUE, sep=",")
human <- read.table ("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human2.txt", header=TRUE, sep=",")

```

#Chapter 5 - Dimensionality reduction techniques
##### The Human Development Index dataset was created to emphasize that people and their capabilities should be the ultimate criteria for assessing the development of a country, not economic growth alone. Here, I have wrangled this data into a more concise form (according to the aims of the chapter 5 exercise) and named it "human".
 

### 1. Preliminary explorations of the data
#### **2.1 The structure and the dimensions of the data**
Below is the structure of the human dataset. The following characteristics of the dataframe can be discerned.

* 155 rows
* 8 variables
    * Datatypes: numeric and int (GD: the GNI is Factor --> check whether this should be corrected in the .R file)




```{r, echo=FALSE}

dim(human)
str(human)

```
<!--Show a graphical overview of the data and show summaries of the variables in the data. Describe and interpret the outputs, commenting on the distributions of the variables and the relationships between them. (0-2 points)-->

#### **2.2 The summary of the data**
Here we'll print out the summary of the data with the *summary()* function to get a grasp of the min, max, median, mean and quantiles of the data.

```{r, echo=FALSE}
summary(human)

```

Here are the standard deviations of the variables.
(Add some analysis here)

```{r, echo=FALSE}
# Lasketaan muuttujien keskihajonnat (tehdään tämä uudestaan, kun muuttujat on skaalattu)
summarise (human, sd(Edu2.FM),sd(Labo.FM),sd(Edu.Exp),sd(Life.Exp),sd(GNI),sd(Mat.Mor),sd(Ado.Birth),sd(Parli.F))



#B1 <- Boston
## Next we'll print out the summary
#summary(Boston)

#group_by(set) %>%
 # summarize(N = n(), Mx= mean(x),Sdx= sd(x), My=mean(y), Sdy=sd(y), cor(x,y))


```

#### **2.3 The graphical overview of the summarized data**
First we'll produce a matrix plot with the basic packages pairs.

```{r, echo=FALSE}
# Then we'll take a look at the graphical overview
pairs(human)
#ggpairs(human)



```

#### **2.4 The correlations of the data with corrplot()**

Now it's time to get a picture of the correlations with the  *cor()* function. Here the correlations were rounded to two desimals to save space.

```{r, echo=FALSE}

cor_matrix<-cor(human)%>% round(2)
cor_matrix


```

#### **2.5 The graphical overview of correlations with the advanced corrplot() function**

The visualization of the correlation matrix with the advanced *corrplot()* function. To reduce repetition, we'll visualize only the upper part of the plot (as is well known, the top part of the correlation matrix contains the same correlations as the bottom part)

```{r, echo=FALSE}
# Now we'll visualize the correlation matrix. To reduce repetition, we'll visualize only the upper part of the plot (as is well known, the top part of the correlation matrix contains the same correlations as the bottom part)
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos = "d", tl.cex=0.6)


```

The above summaries and visualizations showed the following.

* There was a mildly strong positive correlation between crime and index of accessibility to radial highways (rad).
* There was a moderate positive correlation between crime and the following variables.
    * correlations here
    * correlations here
    * lcorrelations here
    * correlations here
* Comment here
* Comment here
      * correlations here
    * correlations here
    * lcorrelations here
    * correlations here
* Comment here
    ** correlations here
    * correlations here
    * lcorrelations here
    * correlations here
    
      


### More plots here (boxplots, density plots, bar plots)




```{r}
# 
# boxplots

```

Let's look at the summaries of the scaled variables and see how the variables changed ( e.g. the means are now all at zero)

```{r, echo=FALSE}

```

bar plots

```{r, echo=FALSE}
# library(dplyr)
# 
#  boston_scaled %>%
#   summarise(sd(crim),
#             sd(zn), sd(indus), sd(chas), sd(nox), sd(rm), sd(age), sd(dis), sd(rad), sd(tax),sd(ptratio), sd(black), sd(lstat), sd(medv))
```

    
### **Performing the Principal component analysis (PCA)**

Next we'll perform principal component analysis (PCA) on the not standardized human data and show variability captured by the principal components.
```{r, echo=FALSE, message= FALSE, warning=FALSE}
### PCA on the non standardized human data
pca_human <- prcomp(human)
s <- summary(pca_human)

pca_pr <- round(100*s$importance[2, ], digits = 1)
pca_pr
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")

## puuttuuko jotain dataa, kun ilmoittaa zero-lenght

```

Then we'll draw a biplot displaying the observations by the first two principal components (PC1 coordinate in x-axis, PC2 coordinate in y-axis), along with arrows representing the original variables. (0-2 points)
```{r, echo=FALSE, message= FALSE, warning=FALSE}

# bibplot of first two principal components
# draw a biplot of the principal component representation and the original variables
biplot(pca_human, choices = 1:2, cex=c(1.2 ,1.2), col= c("grey40", "deeppink2"), xlab = pc_lab[1] , ylab = pc_lab[2])

```

Standardize the variables in the human data and repeat the above analysis. Interpret the results of both analysis (with and without standardizing). Are the results different? Why or why not? Include captions in you plots where you describe the results by using not just your variable names, but the actual phenomenons they relate to. (0-4 points)

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# standardize the variables
human_std <- scale(human)
summary(human_std)
str(human_std)

```
```{r, echo=FALSE}

# Lasketaan standardoitujen muuttujien keskihajonnat
#summarise(human_std_mat, sd(Edu2.FM),sd(Labo.FM),sd(Edu.Exp),sd(Life.Exp),sd(GNI),sd(Mat.Mor),sd(Ado.Birth),sd(Parli.F)

```
```{r, echo=FALSE}

# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human_std)
s <- summary(pca_human)
pca_pr <- round(100*s$importance[2, ], digits = 1)
pca_pr
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")

# draw a biplot of the principal component representation and the original variables
## remember to add caption names (in place of variable names) to the plots describing the phenomenons


#biplot(pca_human, choices = 1:2, cex=c(1 ,1.2), col= c("grey40", "deeppink2"))

biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1] , ylab = pc_lab[2])
```

Let's interpret the results of both analysis (with and without standardizing). Are the results different? Why or why not? Including captions in the plots where I describe the results by using not just your variable names, but the actual phenomenons they relate to. (0-4 points)

Give your personal interpretations of the first two principal component dimensions based on the biplot drawn after PCA on the standardized human data. (0-2 points)


```{r}

# boston_scaled <- dplyr::select(boston_scaled, -crim)
# boston_scaled <- data.frame(boston_scaled, crime)
# table(crime)
```

My personal interpretations of the first two principal component dimensions based on the biplot drawn after PCA on the standardized human data. (0-2 points)



Next we'll load the tea dataset from the package Factominer. Explore the data briefly: look at the structure and the dimensions of the data and visualize it. Then do Multiple Correspondence Analysis on the tea data (or to a certain columns of the data, it’s up to you). Interpret the results of the MCA and draw at least the variable biplot of the analysis. You can also explore other plotting options for MCA. Comment on the output of the plots. (0-4 points)

```{r}
# Install Factominer and load in the tea dataset
#install.packages("FactoMineR")
library(FactoMineR)
data(tea)

```
Let's explore the data briefly: look at the structure and the dimensions of the data and visualize it.

```{r, echo=FALSE}
str(tea)
dim(tea)
#ggpairs(tea)

```




````{r, echo=FALSE}

```


Some final comments here

```{r, echo=FALSE}

```


```{r, echo=FALSE}

```


```{r,echo=FALSE}


```



```{r, echo=FALSE}

```

```{r,echo=FALSE}

```



```{r, echo=FALSE}

```



```{r,echo=FALSE}


```


